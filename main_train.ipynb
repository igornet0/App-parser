{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed41cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install matplotlib\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install sktime\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install transformers\n",
    "# !pip install pytesseract\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b0b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import data_manager, DataManager, settings\n",
    "from core.utils import setup_logging\n",
    "from pathlib import Path\n",
    "\n",
    "settings.logging.level = \"DEBUG\"\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4400e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_manager.backup_data(paths=[Path(\"/Users/igor/Desktop/App-parser/data/raw/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d27560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = {}\n",
    "\n",
    "for coin in data_manager.coin_list:\n",
    "    if coin != \"TON\":\n",
    "        continue\n",
    "    for path in data_manager.get_path(data_type=\"raw\",\n",
    "                                      coin=coin,\n",
    "                                      timetravel=\"5m\"):\n",
    "        coins.setdefault(coin, [])\n",
    "        coins[coin].append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa51dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/Library/Caches/pypoetry/virtualenvs/postgresdocker-MncL8gKv-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from backend.Dataset import DatasetTimeseries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12848394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TON': [PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_1/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_13/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_15/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_5/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_2/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_3/TON/TON_5m.csv'),\n",
       "  PosixPath('/Users/igor/Desktop/App-parser/data/raw/launch_parser_4/TON/TON_5m.csv')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06679c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function clear_dataset executed in 0:00:00.007170\n",
      "Function clear_dataset executed in 0:00:00.275226\n",
      "Function clear_dataset executed in 0:00:02.649099\n",
      "Function clear_dataset executed in 0:00:00.019079\n",
      "Function clear_dataset executed in 0:00:00.010336\n",
      "Function clear_dataset executed in 0:00:00.009349\n",
      "Function clear_dataset executed in 0:00:00.009133\n",
      "Function clear_dataset executed in 0:00:02.682958\n",
      "Directory created: /Users/igor/Desktop/App-parser/data/processed/TON\n",
      "Directory created: /Users/igor/Desktop/App-parser/data/processed/TON/5m\n",
      "Dataset saved to /Users/igor/Desktop/App-parser/data/processed/TON/5m/clear-TON-5m.csv\n"
     ]
    }
   ],
   "source": [
    "coins_dt = {}\n",
    "coins_dubl = {}\n",
    "for coin, paths in coins.items():\n",
    "    for path in paths:\n",
    "        dt = DatasetTimeseries(path)\n",
    "        dt.clear_dataset()\n",
    "        coins_dt.setdefault(coin, [])\n",
    "        coins_dt[coin].append(dt)\n",
    "\n",
    "    df_new = pd.concat(map(lambda dt: dt.get_dataset(), coins_dt[coin]), ignore_index=True)\n",
    "    df_new = DatasetTimeseries(df_new).dataset_clear()\n",
    "    df_new.drop_duplicates(subset=[\"datetime\", \"open\", \"min\", \"max\", \"close\", \"volume\"], inplace=True)\n",
    "    df_new = DatasetTimeseries(df_new)\n",
    "    coins_dubl.setdefault(coin, len(df_new.duplicated()))\n",
    "\n",
    "    df_new.set_dataset(df_new.clear_dataset())\n",
    "    path_coin = data_manager.create_dir(\"processed\", coin)\n",
    "    df_new.set_path_save(data_manager.create_dir(\"processed\", path_coin / \"5m\"))\n",
    "    df_new.save_dataset(f\"clear-{coin}-5m.csv\")\n",
    "    coins_dt[coin] = df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9062ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coin, count in coins_dubl.items():\n",
    "    if count == 0:\n",
    "        continue\n",
    "    print(f\"Coin: {coin}, Duplicates: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a542158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coin='TON'\n",
      "dt.get_dataset().shape=(100213, 6)\n",
      "dt.get_dataset_Nan().shape=(749, 6)\n",
      "0.007474080209154501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for coin, dt in coins_dt.items():\n",
    "    if (len(dt.get_dataset_Nan()) / len(dt.get_dataset())) < 0.3:\n",
    "        print(f\"{coin=}\\n{dt.get_dataset().shape=}\\n{dt.get_dataset_Nan().shape=}\")\n",
    "        print(len(dt.get_dataset_Nan()) / len(dt.get_dataset()))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35eedbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_TON = coins_dt[\"AVAX\"]\n",
    "# df: pd.DataFrame = dt_TON.get_dataset()\n",
    "# # dubl = df.duplicated(subset=[\"datetime\"], keep=False)\n",
    "# for data in dt_TON:\n",
    "#     if data[\"volume\"] != \"x\" and data[\"volume\"] > 100:\n",
    "#         print(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb0797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.MMM import Agent, AgentPReadTime\n",
    "from backend.Dataset import Indicators\n",
    "from backend.train_models import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1350211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(bath: pd.DataFrame) -> pd.DataFrame:\n",
    "    bath[\"SMA20\"] = Indicators.sma(bath, 20)\n",
    "    bath['EMA20'] = Indicators.ema(bath, 20)\n",
    "    bath['RSI14'] = Indicators.rsi(bath)\n",
    "    bath['MACD'], bath['Signal'] = Indicators.macd(bath)\n",
    "    bath['UpperBB'], bath['MiddleBB'], bath['LowerBB'] = Indicators.bollinger_bands(bath)\n",
    "    bath['ATR14'] = Indicators.atr(bath)\n",
    "    bath['%K'], bath['%D'] = Indicators.stochastic_oscillator(bath)\n",
    "   \n",
    "    return bath\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1da8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TON': <backend.Dataset.dataset.DatasetTimeseries at 0x172323f50>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coins_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0bd5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/Library/Caches/pypoetry/virtualenvs/postgresdocker-MncL8gKv-py3.12/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Loading Agent: agent_pred_train\n",
      "Loading schema for: agent_pred_train\n"
     ]
    }
   ],
   "source": [
    "coin_loader = {}\n",
    "for coin, dt in coins_dt.items():\n",
    "    dt: DatasetTimeseries\n",
    "    filter_func = lambda x: x[\"open\"] != \"x\"\n",
    "\n",
    "    loader = dt.get_time_line_loader(time_line_size=100, \n",
    "                                filter_data=filter_func)\n",
    "    \n",
    "    coin_loader[coin] = loader\n",
    "\n",
    "loader_train = Loader(\"agent_pred_train\")\n",
    "agent_manager = loader_train.load_model(count_agents=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5544a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_manager.agent[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c89dec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ Starting ensemble training\n",
      "üèãÔ∏è Training Agent 1:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Mixed precision –Ω–∞ MPS –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igor/Library/Caches/pypoetry/virtualenvs/postgresdocker-MncL8gKv-py3.12/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/igor/Desktop/App-parser/backend/train_models/loader.py:67: UserWarning: Using a target size (torch.Size([30, 5, 16])) that is different to the input size (torch.Size([30, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  price_loss = F.mse_loss(price_pred, targets)\n",
      "üèãÔ∏è Training Agent 1:   0%|          | 0/1000 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mloader_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoin_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43magent_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/App-parser/backend/train_models/loader.py:379\u001b[39m, in \u001b[36mLoader.train_model\u001b[39m\u001b[34m(self, loaders, agent_manager, mixed)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m (pbar := tqdm(agent_manager.agent, desc=\u001b[33m\"\u001b[39m\u001b[33mAgents\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    378\u001b[39m     pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müèãÔ∏è Training Agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_single_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ All agents trained successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/App-parser/backend/train_models/loader.py:318\u001b[39m, in \u001b[36mLoader._train_single_agent\u001b[39m\u001b[34m(self, agent, loaders, epochs, batch_size, base_lr, weight_decay, patience, mixed, mixed_precision)\u001b[39m\n\u001b[32m    315\u001b[39m     outputs = model(x, time_x)\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# print(\"outputs shape:\", outputs.shape)\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# print(\"y shape:\", y.shape)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorized_quantile_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effective_mp:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/App-parser/backend/train_models/loader.py:67\u001b[39m, in \u001b[36mLoader.vectorized_quantile_loss\u001b[39m\u001b[34m(predictions, targets)\u001b[39m\n\u001b[32m     64\u001b[39m direction_pred = predictions[..., \u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# –î–æ–±–∞–≤–∏—Ç—å –≤—ã—Ö–æ–¥ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# MSE –¥–ª—è —Ü–µ–Ω—ã\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m price_loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Binary cross-entropy –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\u001b[39;00m\n\u001b[32m     70\u001b[39m true_direction = (targets[:,\u001b[32m1\u001b[39m:] > targets[:,:-\u001b[32m1\u001b[39m]).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/postgresdocker-MncL8gKv-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:3884\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3882\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/postgresdocker-MncL8gKv-py3.12/lib/python3.12/site-packages/torch/functional.py:77\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (5) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "loader_train.train_model(list(coin_loader.values()),\n",
    "                                  agent_manager=agent_manager,\n",
    "                                  mixed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgresdocker-MncL8gKv-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
